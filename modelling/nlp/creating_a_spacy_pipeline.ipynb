{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example spaCy NLP pipeline on [some Reddit post data](https://www.kaggle.com/datasets/mswarbrickjones/reddit-selfposts).\n",
    "\n",
    "Download the data and extract into a `/data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from textacy import preprocessing, extract\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_363/3723939893.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# Register for Pandas functions\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download en_core_web_sm\n",
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6d8knd</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>Remember your command line switches...</td>\n",
       "      <td>Hi there,  &lt;lb&gt;The usual. Long time lerker, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58mbft</td>\n",
       "      <td>teenmom</td>\n",
       "      <td>So what was Matt \"addicted\" to?</td>\n",
       "      <td>Did he ever say what his addiction was or is h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6ti6re</td>\n",
       "      <td>ringdoorbell</td>\n",
       "      <td>Not door bell, but floodlight mount height.</td>\n",
       "      <td>I know this is a sub for the 'Ring Doorbell' b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77sxto</td>\n",
       "      <td>intel</td>\n",
       "      <td>Worried about my 8700k small fft/data stress r...</td>\n",
       "      <td>Prime95 (regardless of version) and OCCT both,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             subreddit   \n",
       "0  6d8knd  talesfromtechsupport  \\\n",
       "1  58mbft               teenmom   \n",
       "2  8f73s7                Harley   \n",
       "3  6ti6re          ringdoorbell   \n",
       "4  77sxto                 intel   \n",
       "\n",
       "                                               title   \n",
       "0             Remember your command line switches...  \\\n",
       "1                    So what was Matt \"addicted\" to?   \n",
       "2                                     No Club Colors   \n",
       "3        Not door bell, but floodlight mount height.   \n",
       "4  Worried about my 8700k small fft/data stress r...   \n",
       "\n",
       "                                            selftext  \n",
       "0  Hi there,  <lb>The usual. Long time lerker, fi...  \n",
       "1  Did he ever say what his addiction was or is h...  \n",
       "2  Funny story. I went to college in Las Vegas. T...  \n",
       "3  I know this is a sub for the 'Ring Doorbell' b...  \n",
       "4  Prime95 (regardless of version) and OCCT both,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post info\n",
    "posts = pd.read_csv('data/rspct.tsv', sep='\\t')\n",
    "\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>in_data</th>\n",
       "      <th>reason_for_exclusion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>whatsthatbook</th>\n",
       "      <td>advice/question</td>\n",
       "      <td>book</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CasualConversation</th>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clairvoyantreadings</th>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecidingToBeBetter</th>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HelpMeFind</th>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          category_1 category_2 category_3  in_data   \n",
       "subreddit                                                             \n",
       "whatsthatbook        advice/question       book        NaN     True  \\\n",
       "CasualConversation   advice/question      broad        NaN    False   \n",
       "Clairvoyantreadings  advice/question      broad        NaN    False   \n",
       "DecidingToBeBetter   advice/question      broad        NaN    False   \n",
       "HelpMeFind           advice/question      broad        NaN    False   \n",
       "\n",
       "                    reason_for_exclusion  \n",
       "subreddit                                 \n",
       "whatsthatbook                        NaN  \n",
       "CasualConversation             too_broad  \n",
       "Clairvoyantreadings            too_broad  \n",
       "DecidingToBeBetter             too_broad  \n",
       "HelpMeFind                     too_broad  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subreddit info\n",
    "subreddit_info = pd.read_csv('data/subreddit_info.csv').set_index(['subreddit'])\n",
    "\n",
    "subreddit_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>in_data</th>\n",
       "      <th>reason_for_exclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6d8knd</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>Remember your command line switches...</td>\n",
       "      <td>Hi there,  &lt;lb&gt;The usual. Long time lerker, fi...</td>\n",
       "      <td>writing/stories</td>\n",
       "      <td>tech support</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58mbft</td>\n",
       "      <td>teenmom</td>\n",
       "      <td>So what was Matt \"addicted\" to?</td>\n",
       "      <td>Did he ever say what his addiction was or is h...</td>\n",
       "      <td>tv_show</td>\n",
       "      <td>teen mom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. T...</td>\n",
       "      <td>autos</td>\n",
       "      <td>harley davidson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6ti6re</td>\n",
       "      <td>ringdoorbell</td>\n",
       "      <td>Not door bell, but floodlight mount height.</td>\n",
       "      <td>I know this is a sub for the 'Ring Doorbell' b...</td>\n",
       "      <td>hardware/tools</td>\n",
       "      <td>doorbells</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77sxto</td>\n",
       "      <td>intel</td>\n",
       "      <td>Worried about my 8700k small fft/data stress r...</td>\n",
       "      <td>Prime95 (regardless of version) and OCCT both,...</td>\n",
       "      <td>electronics</td>\n",
       "      <td>cpu</td>\n",
       "      <td>intel</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             subreddit   \n",
       "0  6d8knd  talesfromtechsupport  \\\n",
       "1  58mbft               teenmom   \n",
       "2  8f73s7                Harley   \n",
       "3  6ti6re          ringdoorbell   \n",
       "4  77sxto                 intel   \n",
       "\n",
       "                                               title   \n",
       "0             Remember your command line switches...  \\\n",
       "1                    So what was Matt \"addicted\" to?   \n",
       "2                                     No Club Colors   \n",
       "3        Not door bell, but floodlight mount height.   \n",
       "4  Worried about my 8700k small fft/data stress r...   \n",
       "\n",
       "                                            selftext       category_1   \n",
       "0  Hi there,  <lb>The usual. Long time lerker, fi...  writing/stories  \\\n",
       "1  Did he ever say what his addiction was or is h...          tv_show   \n",
       "2  Funny story. I went to college in Las Vegas. T...            autos   \n",
       "3  I know this is a sub for the 'Ring Doorbell' b...   hardware/tools   \n",
       "4  Prime95 (regardless of version) and OCCT both,...      electronics   \n",
       "\n",
       "        category_2 category_3  in_data reason_for_exclusion  \n",
       "0     tech support        NaN     True                  NaN  \n",
       "1         teen mom        NaN     True                  NaN  \n",
       "2  harley davidson        NaN     True                  NaN  \n",
       "3        doorbells        NaN     True                  NaN  \n",
       "4              cpu      intel     True                  NaN  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join\n",
    "df = posts.join(subreddit_info, on='subreddit')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_1</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>autos</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>advice/question</td>\n",
       "      <td>18000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>animals</td>\n",
       "      <td>17000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>education</td>\n",
       "      <td>17000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>social_group</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>politics/viewpoint</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>food/drink</td>\n",
       "      <td>15000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>card_game</td>\n",
       "      <td>15000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>stem</td>\n",
       "      <td>14000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hardware/tools</td>\n",
       "      <td>14000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>parenting</td>\n",
       "      <td>13000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>religion/supernatural</td>\n",
       "      <td>13000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>books</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>appearance</td>\n",
       "      <td>11000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>finance/money</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>board_game</td>\n",
       "      <td>9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>meta</td>\n",
       "      <td>9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rpg</td>\n",
       "      <td>7000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>movies</td>\n",
       "      <td>7000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>travel</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               category_1     id\n",
       "5                   autos  20000\n",
       "0         advice/question  18000\n",
       "1                 animals  17000\n",
       "12              education  17000\n",
       "31           social_group  16000\n",
       "25     politics/viewpoint  16000\n",
       "15             food/drink  15000\n",
       "8               card_game  15000\n",
       "34                   stem  14000\n",
       "17         hardware/tools  14000\n",
       "24              parenting  13000\n",
       "28  religion/supernatural  13000\n",
       "7                   books  12000\n",
       "3              appearance  11000\n",
       "14          finance/money  10000\n",
       "6              board_game   9000\n",
       "20                   meta   9000\n",
       "29                    rpg   7000\n",
       "21                 movies   7000\n",
       "35                 travel   5000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See some category 1s that aren't *too* big\n",
    "df.groupby('category_1')['id'].count().reset_index().sort_values('id', ascending=False).head(50).tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>in_data</th>\n",
       "      <th>reason_for_exclusion</th>\n",
       "      <th>text</th>\n",
       "      <th>doc_apply</th>\n",
       "      <th>doc_spacy_preprocess</th>\n",
       "      <th>doc_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8dasa5</td>\n",
       "      <td>harrypotter</td>\n",
       "      <td>Better idea for Fantastic beast adaptation?</td>\n",
       "      <td>I fell asleep watching the Fantastic Beast mov...</td>\n",
       "      <td>books</td>\n",
       "      <td>harry potter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Better idea for Fantastic beast adaptation? I ...</td>\n",
       "      <td>(Better, idea, for, Fantastic, beast, adaptati...</td>\n",
       "      <td>Better idea for Fantastic beast adaptation? I ...</td>\n",
       "      <td>(Better, idea, for, Fantastic, beast, adaptati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6gygi1</td>\n",
       "      <td>dresdenfiles</td>\n",
       "      <td>(No Spoilers) Preview of Peace Talks</td>\n",
       "      <td>Could someone please tell me how many chapters...</td>\n",
       "      <td>books</td>\n",
       "      <td>dresden files</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(No Spoilers) Preview of Peace Talks Could som...</td>\n",
       "      <td>((, No, Spoilers, ), Preview, of, Peace, Talks...</td>\n",
       "      <td>(No Spoilers) Preview of Peace Talks Could som...</td>\n",
       "      <td>((, No, Spoilers, ), Preview, of, Peace, Talks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5xpvak</td>\n",
       "      <td>Parahumans</td>\n",
       "      <td>Looking for 1-2 players for Worm Campaign</td>\n",
       "      <td>Like the title says im looking for 1-2 more pl...</td>\n",
       "      <td>books</td>\n",
       "      <td>parahumans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Looking for 1-2 players for Worm Campaign Like...</td>\n",
       "      <td>(Looking, for, 1, -, 2, players, for, Worm, Ca...</td>\n",
       "      <td>Looking for 1-2 players for Worm Campaign Like...</td>\n",
       "      <td>(Looking, for, 1, -, 2, players, for, Worm, Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7p3vuy</td>\n",
       "      <td>Malazan</td>\n",
       "      <td>Just finished TtH</td>\n",
       "      <td>I just finished Toll the Hounds, and I dont kn...</td>\n",
       "      <td>books</td>\n",
       "      <td>malazan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just finished TtH I just finished Toll the Hou...</td>\n",
       "      <td>(Just, finished, TtH, I, just, finished, Toll,...</td>\n",
       "      <td>Just finished TtH I just finished Toll the Hou...</td>\n",
       "      <td>(Just, finished, TtH, I, just, finished, Toll,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5vkojs</td>\n",
       "      <td>Stormlight_Archive</td>\n",
       "      <td>[TWoK]/[WoR]/[NO SPOILERS] Something that has ...</td>\n",
       "      <td>Here on earth, green/blue eyes are not a domin...</td>\n",
       "      <td>books</td>\n",
       "      <td>cosmere</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[TWoK]/[WoR]/[NO SPOILERS] Something that has ...</td>\n",
       "      <td>([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...</td>\n",
       "      <td>[TWoK]/[WoR]/[NO SPOILERS] Something that has ...</td>\n",
       "      <td>([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id           subreddit   \n",
       "0  8dasa5         harrypotter  \\\n",
       "1  6gygi1        dresdenfiles   \n",
       "2  5xpvak          Parahumans   \n",
       "3  7p3vuy             Malazan   \n",
       "4  5vkojs  Stormlight_Archive   \n",
       "\n",
       "                                               title   \n",
       "0        Better idea for Fantastic beast adaptation?  \\\n",
       "1               (No Spoilers) Preview of Peace Talks   \n",
       "2          Looking for 1-2 players for Worm Campaign   \n",
       "3                                  Just finished TtH   \n",
       "4  [TWoK]/[WoR]/[NO SPOILERS] Something that has ...   \n",
       "\n",
       "                                            selftext category_1   \n",
       "0  I fell asleep watching the Fantastic Beast mov...      books  \\\n",
       "1  Could someone please tell me how many chapters...      books   \n",
       "2  Like the title says im looking for 1-2 more pl...      books   \n",
       "3  I just finished Toll the Hounds, and I dont kn...      books   \n",
       "4  Here on earth, green/blue eyes are not a domin...      books   \n",
       "\n",
       "      category_2 category_3  in_data reason_for_exclusion   \n",
       "0   harry potter        NaN     True                  NaN  \\\n",
       "1  dresden files        NaN     True                  NaN   \n",
       "2     parahumans        NaN     True                  NaN   \n",
       "3        malazan        NaN     True                  NaN   \n",
       "4        cosmere        NaN     True                  NaN   \n",
       "\n",
       "                                                text   \n",
       "0  Better idea for Fantastic beast adaptation? I ...  \\\n",
       "1  (No Spoilers) Preview of Peace Talks Could som...   \n",
       "2  Looking for 1-2 players for Worm Campaign Like...   \n",
       "3  Just finished TtH I just finished Toll the Hou...   \n",
       "4  [TWoK]/[WoR]/[NO SPOILERS] Something that has ...   \n",
       "\n",
       "                                           doc_apply   \n",
       "0  (Better, idea, for, Fantastic, beast, adaptati...  \\\n",
       "1  ((, No, Spoilers, ), Preview, of, Peace, Talks...   \n",
       "2  (Looking, for, 1, -, 2, players, for, Worm, Ca...   \n",
       "3  (Just, finished, TtH, I, just, finished, Toll,...   \n",
       "4  ([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...   \n",
       "\n",
       "                                doc_spacy_preprocess   \n",
       "0  Better idea for Fantastic beast adaptation? I ...  \\\n",
       "1  (No Spoilers) Preview of Peace Talks Could som...   \n",
       "2  Looking for 1-2 players for Worm Campaign Like...   \n",
       "3  Just finished TtH I just finished Toll the Hou...   \n",
       "4  [TWoK]/[WoR]/[NO SPOILERS] Something that has ...   \n",
       "\n",
       "                                           doc_spacy  \n",
       "0  (Better, idea, for, Fantastic, beast, adaptati...  \n",
       "1  ((, No, Spoilers, ), Preview, of, Peace, Talks...  \n",
       "2  (Looking, for, 1, -, 2, players, for, Worm, Ca...  \n",
       "3  (Just, finished, TtH, I, just, finished, Toll,...  \n",
       "4  ([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter to books\n",
    "df = df[df.category_1 == 'books']\n",
    "\n",
    "# Reset index for easier subsetting/sampling\n",
    "df.reset_index(inplace=True)\n",
    "df.drop('index', axis=1, inplace=True)\n",
    "\n",
    "# Filter to top 1k for faster testing\n",
    "df = df.head(1000)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a basic spaCy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>in_data</th>\n",
       "      <th>reason_for_exclusion</th>\n",
       "      <th>text</th>\n",
       "      <th>doc_apply</th>\n",
       "      <th>doc_spacy_preprocess</th>\n",
       "      <th>doc_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8dasa5</td>\n",
       "      <td>harrypotter</td>\n",
       "      <td>Better idea for Fantastic beast adaptation?</td>\n",
       "      <td>I fell asleep watching the Fantastic Beast mov...</td>\n",
       "      <td>books</td>\n",
       "      <td>harry potter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Better idea for Fantastic beast adaptation? I ...</td>\n",
       "      <td>(Better, idea, for, Fantastic, beast, adaptati...</td>\n",
       "      <td>Better idea for Fantastic beast adaptation? I ...</td>\n",
       "      <td>(Better, idea, for, Fantastic, beast, adaptati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6gygi1</td>\n",
       "      <td>dresdenfiles</td>\n",
       "      <td>(No Spoilers) Preview of Peace Talks</td>\n",
       "      <td>Could someone please tell me how many chapters...</td>\n",
       "      <td>books</td>\n",
       "      <td>dresden files</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(No Spoilers) Preview of Peace Talks Could som...</td>\n",
       "      <td>((, No, Spoilers, ), Preview, of, Peace, Talks...</td>\n",
       "      <td>(No Spoilers) Preview of Peace Talks Could som...</td>\n",
       "      <td>((, No, Spoilers, ), Preview, of, Peace, Talks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5xpvak</td>\n",
       "      <td>Parahumans</td>\n",
       "      <td>Looking for 1-2 players for Worm Campaign</td>\n",
       "      <td>Like the title says im looking for 1-2 more pl...</td>\n",
       "      <td>books</td>\n",
       "      <td>parahumans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Looking for 1-2 players for Worm Campaign Like...</td>\n",
       "      <td>(Looking, for, 1, -, 2, players, for, Worm, Ca...</td>\n",
       "      <td>Looking for 1-2 players for Worm Campaign Like...</td>\n",
       "      <td>(Looking, for, 1, -, 2, players, for, Worm, Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7p3vuy</td>\n",
       "      <td>Malazan</td>\n",
       "      <td>Just finished TtH</td>\n",
       "      <td>I just finished Toll the Hounds, and I dont kn...</td>\n",
       "      <td>books</td>\n",
       "      <td>malazan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just finished TtH I just finished Toll the Hou...</td>\n",
       "      <td>(Just, finished, TtH, I, just, finished, Toll,...</td>\n",
       "      <td>Just finished TtH I just finished Toll the Hou...</td>\n",
       "      <td>(Just, finished, TtH, I, just, finished, Toll,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5vkojs</td>\n",
       "      <td>Stormlight_Archive</td>\n",
       "      <td>[TWoK]/[WoR]/[NO SPOILERS] Something that has ...</td>\n",
       "      <td>Here on earth, green/blue eyes are not a domin...</td>\n",
       "      <td>books</td>\n",
       "      <td>cosmere</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[TWoK]/[WoR]/[NO SPOILERS] Something that has ...</td>\n",
       "      <td>([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...</td>\n",
       "      <td>[TWoK]/[WoR]/[NO SPOILERS] Something that has ...</td>\n",
       "      <td>([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id           subreddit   \n",
       "0  8dasa5         harrypotter  \\\n",
       "1  6gygi1        dresdenfiles   \n",
       "2  5xpvak          Parahumans   \n",
       "3  7p3vuy             Malazan   \n",
       "4  5vkojs  Stormlight_Archive   \n",
       "\n",
       "                                               title   \n",
       "0        Better idea for Fantastic beast adaptation?  \\\n",
       "1               (No Spoilers) Preview of Peace Talks   \n",
       "2          Looking for 1-2 players for Worm Campaign   \n",
       "3                                  Just finished TtH   \n",
       "4  [TWoK]/[WoR]/[NO SPOILERS] Something that has ...   \n",
       "\n",
       "                                            selftext category_1   \n",
       "0  I fell asleep watching the Fantastic Beast mov...      books  \\\n",
       "1  Could someone please tell me how many chapters...      books   \n",
       "2  Like the title says im looking for 1-2 more pl...      books   \n",
       "3  I just finished Toll the Hounds, and I dont kn...      books   \n",
       "4  Here on earth, green/blue eyes are not a domin...      books   \n",
       "\n",
       "      category_2 category_3  in_data reason_for_exclusion   \n",
       "0   harry potter        NaN     True                  NaN  \\\n",
       "1  dresden files        NaN     True                  NaN   \n",
       "2     parahumans        NaN     True                  NaN   \n",
       "3        malazan        NaN     True                  NaN   \n",
       "4        cosmere        NaN     True                  NaN   \n",
       "\n",
       "                                                text   \n",
       "0  Better idea for Fantastic beast adaptation? I ...  \\\n",
       "1  (No Spoilers) Preview of Peace Talks Could som...   \n",
       "2  Looking for 1-2 players for Worm Campaign Like...   \n",
       "3  Just finished TtH I just finished Toll the Hou...   \n",
       "4  [TWoK]/[WoR]/[NO SPOILERS] Something that has ...   \n",
       "\n",
       "                                           doc_apply   \n",
       "0  (Better, idea, for, Fantastic, beast, adaptati...  \\\n",
       "1  ((, No, Spoilers, ), Preview, of, Peace, Talks...   \n",
       "2  (Looking, for, 1, -, 2, players, for, Worm, Ca...   \n",
       "3  (Just, finished, TtH, I, just, finished, Toll,...   \n",
       "4  ([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...   \n",
       "\n",
       "                                doc_spacy_preprocess   \n",
       "0  Better idea for Fantastic beast adaptation? I ...  \\\n",
       "1  (No Spoilers) Preview of Peace Talks Could som...   \n",
       "2  Looking for 1-2 players for Worm Campaign Like...   \n",
       "3  Just finished TtH I just finished Toll the Hou...   \n",
       "4  [TWoK]/[WoR]/[NO SPOILERS] Something that has ...   \n",
       "\n",
       "                                           doc_spacy  \n",
       "0  (Better, idea, for, Fantastic, beast, adaptati...  \n",
       "1  ((, No, Spoilers, ), Preview, of, Peace, Talks...  \n",
       "2  (Looking, for, 1, -, 2, players, for, Worm, Ca...  \n",
       "3  (Just, finished, TtH, I, just, finished, Toll,...  \n",
       "4  ([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all text in column\n",
    "df['text'] = df['title'] + ' ' + df['selftext']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f875938a140>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f87591b5b40>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f87597eb300>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f8759100540>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7f8759140f40>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f87597ebca0>)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to process\n",
    "def process_text(text):\n",
    "    # Remove html tags like <lb>\n",
    "    text = re.sub('<[^<>]*>', '', text)\n",
    "    \n",
    "    # Mask emails and phone numbers\n",
    "    text = preprocessing.replace.emails(text)\n",
    "    text = preprocessing.replace.phone_numbers(text)\n",
    "    \n",
    "    # NLP it\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for displayign results\n",
    "def display_nlp(doc):\n",
    "    rows = []\n",
    "    for i, t in enumerate(doc):\n",
    "        if not t.is_punct:\n",
    "            row = {'token': i, 'text': t.text, 'lemma_': t.lemma_,\n",
    "                'is_stop': t.is_stop, 'is_alpha': t.is_alpha, 'pos_': t.pos_,\n",
    "                'dep_': t.dep_, 'ent_type_': t.ent_type_, 'ent_iob_': t.ent_iob_, \n",
    "                'ent_id': t.ent_id_, 'like_email_': t.like_email\n",
    "            }\n",
    "            rows.append(row)\n",
    "    df = pd.DataFrame(rows).set_index('token')\n",
    "    df.index.name = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process a single record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Better idea for Fantastic beast adaptation? I fell asleep watching the Fantastic Beast movie so I’m guessing it wasn’t great.  I feel like it would be really well adapted into a TV show (not much knowledge on media rights).  Each chapter would be an episode and use different characters to show the discovery or a crazy moment involving the creature.  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One record\n",
    "doc = process_text(df.text[0])\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma_</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>pos_</th>\n",
       "      <th>dep_</th>\n",
       "      <th>ent_type_</th>\n",
       "      <th>ent_iob_</th>\n",
       "      <th>ent_id</th>\n",
       "      <th>like_email_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Better</td>\n",
       "      <td>well</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>idea</td>\n",
       "      <td>idea</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fantastic</td>\n",
       "      <td>fantastic</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>NORP</td>\n",
       "      <td>B</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beast</td>\n",
       "      <td>beast</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>compound</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adaptation</td>\n",
       "      <td>adaptation</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>pobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fell</td>\n",
       "      <td>fall</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>asleep</td>\n",
       "      <td>asleep</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>advmod</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>watching</td>\n",
       "      <td>watch</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>advcl</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Fantastic</td>\n",
       "      <td>Fantastic</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>compound</td>\n",
       "      <td>LOC</td>\n",
       "      <td>B</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Beast</td>\n",
       "      <td>Beast</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>compound</td>\n",
       "      <td>LOC</td>\n",
       "      <td>I</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>movie</td>\n",
       "      <td>movie</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>dobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>so</td>\n",
       "      <td>so</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>mark</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>’m</td>\n",
       "      <td>’m</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>AUX</td>\n",
       "      <td>aux</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>guessing</td>\n",
       "      <td>guess</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>advcl</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td>ccomp</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>n’t</td>\n",
       "      <td>not</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>PART</td>\n",
       "      <td>neg</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>great</td>\n",
       "      <td>great</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>acomp</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>dep</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>feel</td>\n",
       "      <td>feel</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>like</td>\n",
       "      <td>like</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>mark</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>would</td>\n",
       "      <td>would</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td>aux</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td>auxpass</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>really</td>\n",
       "      <td>really</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADV</td>\n",
       "      <td>advmod</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>well</td>\n",
       "      <td>well</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADV</td>\n",
       "      <td>advmod</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>adapted</td>\n",
       "      <td>adapt</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>advcl</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>into</td>\n",
       "      <td>into</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>TV</td>\n",
       "      <td>tv</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>compound</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>show</td>\n",
       "      <td>show</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>pobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>not</td>\n",
       "      <td>not</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PART</td>\n",
       "      <td>neg</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>much</td>\n",
       "      <td>much</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>knowledge</td>\n",
       "      <td>knowledge</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>appos</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>media</td>\n",
       "      <td>medium</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>compound</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>rights</td>\n",
       "      <td>right</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>pobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>dep</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Each</td>\n",
       "      <td>each</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>chapter</td>\n",
       "      <td>chapter</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>would</td>\n",
       "      <td>would</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td>aux</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>episode</td>\n",
       "      <td>episode</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>attr</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>cc</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>conj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>different</td>\n",
       "      <td>different</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>characters</td>\n",
       "      <td>character</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>dobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PART</td>\n",
       "      <td>aux</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>show</td>\n",
       "      <td>show</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>xcomp</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>discovery</td>\n",
       "      <td>discovery</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>dobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>or</td>\n",
       "      <td>or</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>cc</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>crazy</td>\n",
       "      <td>crazy</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>moment</td>\n",
       "      <td>moment</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>conj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>involving</td>\n",
       "      <td>involve</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>acl</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>creature</td>\n",
       "      <td>creature</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>dobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>dep</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text      lemma_  is_stop  is_alpha   pos_      dep_ ent_type_   \n",
       "0       Better        well    False      True    ADJ      amod            \\\n",
       "1         idea        idea    False      True   NOUN      ROOT             \n",
       "2          for         for     True      True    ADP      prep             \n",
       "3    Fantastic   fantastic    False      True    ADJ      amod      NORP   \n",
       "4        beast       beast    False      True   NOUN  compound             \n",
       "5   adaptation  adaptation    False      True   NOUN      pobj             \n",
       "7            I           I     True      True   PRON     nsubj             \n",
       "8         fell        fall    False      True   VERB      ROOT             \n",
       "9       asleep      asleep    False      True    ADJ    advmod             \n",
       "10    watching       watch    False      True   VERB     advcl             \n",
       "11         the         the     True      True    DET       det             \n",
       "12   Fantastic   Fantastic    False      True  PROPN  compound       LOC   \n",
       "13       Beast       Beast    False      True  PROPN  compound       LOC   \n",
       "14       movie       movie    False      True   NOUN      dobj             \n",
       "15          so          so     True      True  SCONJ      mark             \n",
       "16           I           I     True      True   PRON     nsubj             \n",
       "17          ’m          ’m     True     False    AUX       aux             \n",
       "18    guessing       guess    False      True   VERB     advcl             \n",
       "19          it          it     True      True   PRON     nsubj             \n",
       "20         was          be     True      True    AUX     ccomp             \n",
       "21         n’t         not     True     False   PART       neg             \n",
       "22       great       great    False      True    ADJ     acomp             \n",
       "24                            False     False  SPACE       dep             \n",
       "25           I           I     True      True   PRON     nsubj             \n",
       "26        feel        feel    False      True   VERB      ROOT             \n",
       "27        like        like    False      True  SCONJ      mark             \n",
       "28          it          it     True      True   PRON     nsubj             \n",
       "29       would       would     True      True    AUX       aux             \n",
       "30          be          be     True      True    AUX   auxpass             \n",
       "31      really      really     True      True    ADV    advmod             \n",
       "32        well        well     True      True    ADV    advmod             \n",
       "33     adapted       adapt    False      True   VERB     advcl             \n",
       "34        into        into     True      True    ADP      prep             \n",
       "35           a           a     True      True    DET       det             \n",
       "36          TV          tv    False      True   NOUN  compound             \n",
       "37        show        show     True      True   NOUN      pobj             \n",
       "39         not         not     True      True   PART       neg             \n",
       "40        much        much     True      True    ADJ      amod             \n",
       "41   knowledge   knowledge    False      True   NOUN     appos             \n",
       "42          on          on     True      True    ADP      prep             \n",
       "43       media      medium    False      True   NOUN  compound             \n",
       "44      rights       right    False      True   NOUN      pobj             \n",
       "47                            False     False  SPACE       dep             \n",
       "48        Each        each     True      True    DET       det             \n",
       "49     chapter     chapter    False      True   NOUN     nsubj             \n",
       "50       would       would     True      True    AUX       aux             \n",
       "51          be          be     True      True    AUX      ROOT             \n",
       "52          an          an     True      True    DET       det             \n",
       "53     episode     episode    False      True   NOUN      attr             \n",
       "54         and         and     True      True  CCONJ        cc             \n",
       "55         use         use    False      True   VERB      conj             \n",
       "56   different   different    False      True    ADJ      amod             \n",
       "57  characters   character    False      True   NOUN      dobj             \n",
       "58          to          to     True      True   PART       aux             \n",
       "59        show        show     True      True   VERB     xcomp             \n",
       "60         the         the     True      True    DET       det             \n",
       "61   discovery   discovery    False      True   NOUN      dobj             \n",
       "62          or          or     True      True  CCONJ        cc             \n",
       "63           a           a     True      True    DET       det             \n",
       "64       crazy       crazy    False      True    ADJ      amod             \n",
       "65      moment      moment    False      True   NOUN      conj             \n",
       "66   involving     involve    False      True   VERB       acl             \n",
       "67         the         the     True      True    DET       det             \n",
       "68    creature    creature    False      True   NOUN      dobj             \n",
       "70                            False     False  SPACE       dep             \n",
       "\n",
       "   ent_iob_ ent_id  like_email_  \n",
       "0         O               False  \n",
       "1         O               False  \n",
       "2         O               False  \n",
       "3         B               False  \n",
       "4         O               False  \n",
       "5         O               False  \n",
       "7         O               False  \n",
       "8         O               False  \n",
       "9         O               False  \n",
       "10        O               False  \n",
       "11        O               False  \n",
       "12        B               False  \n",
       "13        I               False  \n",
       "14        O               False  \n",
       "15        O               False  \n",
       "16        O               False  \n",
       "17        O               False  \n",
       "18        O               False  \n",
       "19        O               False  \n",
       "20        O               False  \n",
       "21        O               False  \n",
       "22        O               False  \n",
       "24        O               False  \n",
       "25        O               False  \n",
       "26        O               False  \n",
       "27        O               False  \n",
       "28        O               False  \n",
       "29        O               False  \n",
       "30        O               False  \n",
       "31        O               False  \n",
       "32        O               False  \n",
       "33        O               False  \n",
       "34        O               False  \n",
       "35        O               False  \n",
       "36        O               False  \n",
       "37        O               False  \n",
       "39        O               False  \n",
       "40        O               False  \n",
       "41        O               False  \n",
       "42        O               False  \n",
       "43        O               False  \n",
       "44        O               False  \n",
       "47        O               False  \n",
       "48        O               False  \n",
       "49        O               False  \n",
       "50        O               False  \n",
       "51        O               False  \n",
       "52        O               False  \n",
       "53        O               False  \n",
       "54        O               False  \n",
       "55        O               False  \n",
       "56        O               False  \n",
       "57        O               False  \n",
       "58        O               False  \n",
       "59        O               False  \n",
       "60        O               False  \n",
       "61        O               False  \n",
       "62        O               False  \n",
       "63        O               False  \n",
       "64        O               False  \n",
       "65        O               False  \n",
       "66        O               False  \n",
       "67        O               False  \n",
       "68        O               False  \n",
       "70        O               False  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "display_nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process all records in a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First using tqdm's progress_apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91edf7941f9c482792824151b5d43ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['doc_apply'] = df['text'].progress_apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>in_data</th>\n",
       "      <th>reason_for_exclusion</th>\n",
       "      <th>text</th>\n",
       "      <th>doc_apply</th>\n",
       "      <th>doc_spacy_preprocess</th>\n",
       "      <th>doc_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8dasa5</td>\n",
       "      <td>harrypotter</td>\n",
       "      <td>Better idea for Fantastic beast adaptation?</td>\n",
       "      <td>I fell asleep watching the Fantastic Beast mov...</td>\n",
       "      <td>books</td>\n",
       "      <td>harry potter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Better idea for Fantastic beast adaptation? I ...</td>\n",
       "      <td>(Better, idea, for, Fantastic, beast, adaptati...</td>\n",
       "      <td>Better idea for Fantastic beast adaptation? I ...</td>\n",
       "      <td>(Better, idea, for, Fantastic, beast, adaptati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6gygi1</td>\n",
       "      <td>dresdenfiles</td>\n",
       "      <td>(No Spoilers) Preview of Peace Talks</td>\n",
       "      <td>Could someone please tell me how many chapters...</td>\n",
       "      <td>books</td>\n",
       "      <td>dresden files</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(No Spoilers) Preview of Peace Talks Could som...</td>\n",
       "      <td>((, No, Spoilers, ), Preview, of, Peace, Talks...</td>\n",
       "      <td>(No Spoilers) Preview of Peace Talks Could som...</td>\n",
       "      <td>((, No, Spoilers, ), Preview, of, Peace, Talks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5xpvak</td>\n",
       "      <td>Parahumans</td>\n",
       "      <td>Looking for 1-2 players for Worm Campaign</td>\n",
       "      <td>Like the title says im looking for 1-2 more pl...</td>\n",
       "      <td>books</td>\n",
       "      <td>parahumans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Looking for 1-2 players for Worm Campaign Like...</td>\n",
       "      <td>(Looking, for, 1, -, 2, players, for, Worm, Ca...</td>\n",
       "      <td>Looking for 1-2 players for Worm Campaign Like...</td>\n",
       "      <td>(Looking, for, 1, -, 2, players, for, Worm, Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7p3vuy</td>\n",
       "      <td>Malazan</td>\n",
       "      <td>Just finished TtH</td>\n",
       "      <td>I just finished Toll the Hounds, and I dont kn...</td>\n",
       "      <td>books</td>\n",
       "      <td>malazan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just finished TtH I just finished Toll the Hou...</td>\n",
       "      <td>(Just, finished, TtH, I, just, finished, Toll,...</td>\n",
       "      <td>Just finished TtH I just finished Toll the Hou...</td>\n",
       "      <td>(Just, finished, TtH, I, just, finished, Toll,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5vkojs</td>\n",
       "      <td>Stormlight_Archive</td>\n",
       "      <td>[TWoK]/[WoR]/[NO SPOILERS] Something that has ...</td>\n",
       "      <td>Here on earth, green/blue eyes are not a domin...</td>\n",
       "      <td>books</td>\n",
       "      <td>cosmere</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[TWoK]/[WoR]/[NO SPOILERS] Something that has ...</td>\n",
       "      <td>([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...</td>\n",
       "      <td>[TWoK]/[WoR]/[NO SPOILERS] Something that has ...</td>\n",
       "      <td>([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id           subreddit   \n",
       "0  8dasa5         harrypotter  \\\n",
       "1  6gygi1        dresdenfiles   \n",
       "2  5xpvak          Parahumans   \n",
       "3  7p3vuy             Malazan   \n",
       "4  5vkojs  Stormlight_Archive   \n",
       "\n",
       "                                               title   \n",
       "0        Better idea for Fantastic beast adaptation?  \\\n",
       "1               (No Spoilers) Preview of Peace Talks   \n",
       "2          Looking for 1-2 players for Worm Campaign   \n",
       "3                                  Just finished TtH   \n",
       "4  [TWoK]/[WoR]/[NO SPOILERS] Something that has ...   \n",
       "\n",
       "                                            selftext category_1   \n",
       "0  I fell asleep watching the Fantastic Beast mov...      books  \\\n",
       "1  Could someone please tell me how many chapters...      books   \n",
       "2  Like the title says im looking for 1-2 more pl...      books   \n",
       "3  I just finished Toll the Hounds, and I dont kn...      books   \n",
       "4  Here on earth, green/blue eyes are not a domin...      books   \n",
       "\n",
       "      category_2 category_3  in_data reason_for_exclusion   \n",
       "0   harry potter        NaN     True                  NaN  \\\n",
       "1  dresden files        NaN     True                  NaN   \n",
       "2     parahumans        NaN     True                  NaN   \n",
       "3        malazan        NaN     True                  NaN   \n",
       "4        cosmere        NaN     True                  NaN   \n",
       "\n",
       "                                                text   \n",
       "0  Better idea for Fantastic beast adaptation? I ...  \\\n",
       "1  (No Spoilers) Preview of Peace Talks Could som...   \n",
       "2  Looking for 1-2 players for Worm Campaign Like...   \n",
       "3  Just finished TtH I just finished Toll the Hou...   \n",
       "4  [TWoK]/[WoR]/[NO SPOILERS] Something that has ...   \n",
       "\n",
       "                                           doc_apply   \n",
       "0  (Better, idea, for, Fantastic, beast, adaptati...  \\\n",
       "1  ((, No, Spoilers, ), Preview, of, Peace, Talks...   \n",
       "2  (Looking, for, 1, -, 2, players, for, Worm, Ca...   \n",
       "3  (Just, finished, TtH, I, just, finished, Toll,...   \n",
       "4  ([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...   \n",
       "\n",
       "                                doc_spacy_preprocess   \n",
       "0  Better idea for Fantastic beast adaptation? I ...  \\\n",
       "1  (No Spoilers) Preview of Peace Talks Could som...   \n",
       "2  Looking for 1-2 players for Worm Campaign Like...   \n",
       "3  Just finished TtH I just finished Toll the Hou...   \n",
       "4  [TWoK]/[WoR]/[NO SPOILERS] Something that has ...   \n",
       "\n",
       "                                           doc_spacy  \n",
       "0  (Better, idea, for, Fantastic, beast, adaptati...  \n",
       "1  ((, No, Spoilers, ), Preview, of, Peace, Talks...  \n",
       "2  (Looking, for, 1, -, 2, players, for, Worm, Ca...  \n",
       "3  (Just, finished, TtH, I, just, finished, Toll,...  \n",
       "4  ([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma_</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>pos_</th>\n",
       "      <th>dep_</th>\n",
       "      <th>ent_type_</th>\n",
       "      <th>ent_iob_</th>\n",
       "      <th>ent_id</th>\n",
       "      <th>like_email_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>compound</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spoilers</td>\n",
       "      <td>Spoilers</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>nmod</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Preview</td>\n",
       "      <td>Preview</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Peace</td>\n",
       "      <td>Peace</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>compound</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Talks</td>\n",
       "      <td>Talks</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>pobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Could</td>\n",
       "      <td>could</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td>aux</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>someone</td>\n",
       "      <td>someone</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>please</td>\n",
       "      <td>please</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>intj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tell</td>\n",
       "      <td>tell</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>me</td>\n",
       "      <td>I</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td>dobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>how</td>\n",
       "      <td>how</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>advmod</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>many</td>\n",
       "      <td>many</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>chapters</td>\n",
       "      <td>chapter</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>nsubjpass</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Peace</td>\n",
       "      <td>Peace</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>compound</td>\n",
       "      <td>LOC</td>\n",
       "      <td>B</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Talks</td>\n",
       "      <td>Talks</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>pobj</td>\n",
       "      <td>LOC</td>\n",
       "      <td>I</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>have</td>\n",
       "      <td>have</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td>aux</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>been</td>\n",
       "      <td>be</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td>auxpass</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>revealed</td>\n",
       "      <td>reveal</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ccomp</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>as</td>\n",
       "      <td>as</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>teasers</td>\n",
       "      <td>teaser</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>pobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>think</td>\n",
       "      <td>think</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>have</td>\n",
       "      <td>have</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td>aux</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>read</td>\n",
       "      <td>read</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ccomp</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>first</td>\n",
       "      <td>first</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>ORDINAL</td>\n",
       "      <td>B</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>chapter</td>\n",
       "      <td>chapter</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>dobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>or</td>\n",
       "      <td>or</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>cc</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>atleast</td>\n",
       "      <td>atleast</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>conj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>think</td>\n",
       "      <td>think</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>first</td>\n",
       "      <td>first</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>ORDINAL</td>\n",
       "      <td>B</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>dobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>that</td>\n",
       "      <td>that</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td>nsubjpass</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td>auxpass</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>released</td>\n",
       "      <td>release</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>relcl</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>but</td>\n",
       "      <td>but</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>cc</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>have</td>\n",
       "      <td>have</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td>aux</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>there</td>\n",
       "      <td>there</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td>expl</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>been</td>\n",
       "      <td>be</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td>conj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>others</td>\n",
       "      <td>other</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>attr</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>If</td>\n",
       "      <td>if</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>mark</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>so</td>\n",
       "      <td>so</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADV</td>\n",
       "      <td>advcl</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>could</td>\n",
       "      <td>could</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td>aux</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>you</td>\n",
       "      <td>you</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>provide</td>\n",
       "      <td>provide</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text    lemma_  is_stop  is_alpha   pos_       dep_ ent_type_   \n",
       "1         No        No     True      True  PROPN   compound            \\\n",
       "2   Spoilers  Spoilers    False      True  PROPN       nmod             \n",
       "4    Preview   Preview    False      True  PROPN      nsubj             \n",
       "5         of        of     True      True    ADP       prep             \n",
       "6      Peace     Peace    False      True  PROPN   compound             \n",
       "7      Talks     Talks    False      True  PROPN       pobj             \n",
       "8      Could     could     True      True    AUX        aux             \n",
       "9    someone   someone     True      True   PRON      nsubj             \n",
       "10    please    please     True      True   INTJ       intj             \n",
       "11      tell      tell    False      True   VERB       ROOT             \n",
       "12        me         I     True      True   PRON       dobj             \n",
       "13       how       how     True      True  SCONJ     advmod             \n",
       "14      many      many     True      True    ADJ       amod             \n",
       "15  chapters   chapter    False      True   NOUN  nsubjpass             \n",
       "16        of        of     True      True    ADP       prep             \n",
       "17     Peace     Peace    False      True  PROPN   compound       LOC   \n",
       "18     Talks     Talks    False      True  PROPN       pobj       LOC   \n",
       "19      have      have     True      True    AUX        aux             \n",
       "20      been        be     True      True    AUX    auxpass             \n",
       "21  revealed    reveal    False      True   VERB      ccomp             \n",
       "22        as        as     True      True    ADP       prep             \n",
       "23   teasers    teaser    False      True   NOUN       pobj             \n",
       "25         I         I     True      True   PRON      nsubj             \n",
       "26     think     think    False      True   VERB       ROOT             \n",
       "27         I         I     True      True   PRON      nsubj             \n",
       "28      have      have     True      True    AUX        aux             \n",
       "29      read      read    False      True   VERB      ccomp             \n",
       "30       the       the     True      True    DET        det             \n",
       "31     first     first     True      True    ADJ       amod   ORDINAL   \n",
       "32   chapter   chapter    False      True   NOUN       dobj             \n",
       "34        or        or     True      True  CCONJ         cc             \n",
       "35   atleast   atleast    False      True   NOUN       conj             \n",
       "36         I         I     True      True   PRON      nsubj             \n",
       "37     think     think    False      True   VERB       ROOT             \n",
       "38       the       the     True      True    DET        det             \n",
       "39     first     first     True      True    ADJ       amod   ORDINAL   \n",
       "40       one       one     True      True   NOUN       dobj             \n",
       "41      that      that     True      True   PRON  nsubjpass             \n",
       "42       was        be     True      True    AUX    auxpass             \n",
       "43  released   release    False      True   VERB      relcl             \n",
       "45       but       but     True      True  CCONJ         cc             \n",
       "46      have      have     True      True    AUX        aux             \n",
       "47     there     there     True      True   PRON       expl             \n",
       "48      been        be     True      True    AUX       conj             \n",
       "49    others     other     True      True   NOUN       attr             \n",
       "51        If        if     True      True  SCONJ       mark             \n",
       "52        so        so     True      True    ADV      advcl             \n",
       "54     could     could     True      True    AUX        aux             \n",
       "55       you       you     True      True   PRON      nsubj             \n",
       "56   provide   provide    False      True   VERB       ROOT             \n",
       "\n",
       "   ent_iob_ ent_id  like_email_  \n",
       "1         O               False  \n",
       "2         O               False  \n",
       "4         O               False  \n",
       "5         O               False  \n",
       "6         O               False  \n",
       "7         O               False  \n",
       "8         O               False  \n",
       "9         O               False  \n",
       "10        O               False  \n",
       "11        O               False  \n",
       "12        O               False  \n",
       "13        O               False  \n",
       "14        O               False  \n",
       "15        O               False  \n",
       "16        O               False  \n",
       "17        B               False  \n",
       "18        I               False  \n",
       "19        O               False  \n",
       "20        O               False  \n",
       "21        O               False  \n",
       "22        O               False  \n",
       "23        O               False  \n",
       "25        O               False  \n",
       "26        O               False  \n",
       "27        O               False  \n",
       "28        O               False  \n",
       "29        O               False  \n",
       "30        O               False  \n",
       "31        B               False  \n",
       "32        O               False  \n",
       "34        O               False  \n",
       "35        O               False  \n",
       "36        O               False  \n",
       "37        O               False  \n",
       "38        O               False  \n",
       "39        B               False  \n",
       "40        O               False  \n",
       "41        O               False  \n",
       "42        O               False  \n",
       "43        O               False  \n",
       "45        O               False  \n",
       "46        O               False  \n",
       "47        O               False  \n",
       "48        O               False  \n",
       "49        O               False  \n",
       "51        O               False  \n",
       "52        O               False  \n",
       "54        O               False  \n",
       "55        O               False  \n",
       "56        O               False  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the tokens for one of them\n",
    "display_nlp(df.loc[1, 'doc_apply']).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, using spacy's batch functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for doing the textacy preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove html tags like <lb>\n",
    "    text = re.sub('<[^<>]*>', '', text)\n",
    "    \n",
    "    # Mask emails and phone numbers\n",
    "    text = preprocessing.replace.emails(text)\n",
    "    text = preprocessing.replace.phone_numbers(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7dd284ac6d4932bab29f8ff8e3410a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['doc_spacy_preprocess'] = df['text'].progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>in_data</th>\n",
       "      <th>reason_for_exclusion</th>\n",
       "      <th>text</th>\n",
       "      <th>doc_apply</th>\n",
       "      <th>doc_spacy_preprocess</th>\n",
       "      <th>doc_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8dasa5</td>\n",
       "      <td>harrypotter</td>\n",
       "      <td>Better idea for Fantastic beast adaptation?</td>\n",
       "      <td>I fell asleep watching the Fantastic Beast mov...</td>\n",
       "      <td>books</td>\n",
       "      <td>harry potter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Better idea for Fantastic beast adaptation? I ...</td>\n",
       "      <td>(Better, idea, for, Fantastic, beast, adaptati...</td>\n",
       "      <td>Better idea for Fantastic beast adaptation? I ...</td>\n",
       "      <td>(Better, idea, for, Fantastic, beast, adaptati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6gygi1</td>\n",
       "      <td>dresdenfiles</td>\n",
       "      <td>(No Spoilers) Preview of Peace Talks</td>\n",
       "      <td>Could someone please tell me how many chapters...</td>\n",
       "      <td>books</td>\n",
       "      <td>dresden files</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(No Spoilers) Preview of Peace Talks Could som...</td>\n",
       "      <td>((, No, Spoilers, ), Preview, of, Peace, Talks...</td>\n",
       "      <td>(No Spoilers) Preview of Peace Talks Could som...</td>\n",
       "      <td>((, No, Spoilers, ), Preview, of, Peace, Talks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5xpvak</td>\n",
       "      <td>Parahumans</td>\n",
       "      <td>Looking for 1-2 players for Worm Campaign</td>\n",
       "      <td>Like the title says im looking for 1-2 more pl...</td>\n",
       "      <td>books</td>\n",
       "      <td>parahumans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Looking for 1-2 players for Worm Campaign Like...</td>\n",
       "      <td>(Looking, for, 1, -, 2, players, for, Worm, Ca...</td>\n",
       "      <td>Looking for 1-2 players for Worm Campaign Like...</td>\n",
       "      <td>(Looking, for, 1, -, 2, players, for, Worm, Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7p3vuy</td>\n",
       "      <td>Malazan</td>\n",
       "      <td>Just finished TtH</td>\n",
       "      <td>I just finished Toll the Hounds, and I dont kn...</td>\n",
       "      <td>books</td>\n",
       "      <td>malazan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just finished TtH I just finished Toll the Hou...</td>\n",
       "      <td>(Just, finished, TtH, I, just, finished, Toll,...</td>\n",
       "      <td>Just finished TtH I just finished Toll the Hou...</td>\n",
       "      <td>(Just, finished, TtH, I, just, finished, Toll,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5vkojs</td>\n",
       "      <td>Stormlight_Archive</td>\n",
       "      <td>[TWoK]/[WoR]/[NO SPOILERS] Something that has ...</td>\n",
       "      <td>Here on earth, green/blue eyes are not a domin...</td>\n",
       "      <td>books</td>\n",
       "      <td>cosmere</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[TWoK]/[WoR]/[NO SPOILERS] Something that has ...</td>\n",
       "      <td>([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...</td>\n",
       "      <td>[TWoK]/[WoR]/[NO SPOILERS] Something that has ...</td>\n",
       "      <td>([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id           subreddit   \n",
       "0  8dasa5         harrypotter  \\\n",
       "1  6gygi1        dresdenfiles   \n",
       "2  5xpvak          Parahumans   \n",
       "3  7p3vuy             Malazan   \n",
       "4  5vkojs  Stormlight_Archive   \n",
       "\n",
       "                                               title   \n",
       "0        Better idea for Fantastic beast adaptation?  \\\n",
       "1               (No Spoilers) Preview of Peace Talks   \n",
       "2          Looking for 1-2 players for Worm Campaign   \n",
       "3                                  Just finished TtH   \n",
       "4  [TWoK]/[WoR]/[NO SPOILERS] Something that has ...   \n",
       "\n",
       "                                            selftext category_1   \n",
       "0  I fell asleep watching the Fantastic Beast mov...      books  \\\n",
       "1  Could someone please tell me how many chapters...      books   \n",
       "2  Like the title says im looking for 1-2 more pl...      books   \n",
       "3  I just finished Toll the Hounds, and I dont kn...      books   \n",
       "4  Here on earth, green/blue eyes are not a domin...      books   \n",
       "\n",
       "      category_2 category_3  in_data reason_for_exclusion   \n",
       "0   harry potter        NaN     True                  NaN  \\\n",
       "1  dresden files        NaN     True                  NaN   \n",
       "2     parahumans        NaN     True                  NaN   \n",
       "3        malazan        NaN     True                  NaN   \n",
       "4        cosmere        NaN     True                  NaN   \n",
       "\n",
       "                                                text   \n",
       "0  Better idea for Fantastic beast adaptation? I ...  \\\n",
       "1  (No Spoilers) Preview of Peace Talks Could som...   \n",
       "2  Looking for 1-2 players for Worm Campaign Like...   \n",
       "3  Just finished TtH I just finished Toll the Hou...   \n",
       "4  [TWoK]/[WoR]/[NO SPOILERS] Something that has ...   \n",
       "\n",
       "                                           doc_apply   \n",
       "0  (Better, idea, for, Fantastic, beast, adaptati...  \\\n",
       "1  ((, No, Spoilers, ), Preview, of, Peace, Talks...   \n",
       "2  (Looking, for, 1, -, 2, players, for, Worm, Ca...   \n",
       "3  (Just, finished, TtH, I, just, finished, Toll,...   \n",
       "4  ([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...   \n",
       "\n",
       "                                doc_spacy_preprocess   \n",
       "0  Better idea for Fantastic beast adaptation? I ...  \\\n",
       "1  (No Spoilers) Preview of Peace Talks Could som...   \n",
       "2  Looking for 1-2 players for Worm Campaign Like...   \n",
       "3  Just finished TtH I just finished Toll the Hou...   \n",
       "4  [TWoK]/[WoR]/[NO SPOILERS] Something that has ...   \n",
       "\n",
       "                                           doc_spacy  \n",
       "0  (Better, idea, for, Fantastic, beast, adaptati...  \n",
       "1  ((, No, Spoilers, ), Preview, of, Peace, Talks...  \n",
       "2  (Looking, for, 1, -, 2, players, for, Worm, Ca...  \n",
       "3  (Just, finished, TtH, I, just, finished, Toll,...  \n",
       "4  ([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_spacy_batch(nlp, df, input_col, output_col, batch_size=50):\n",
    "    #for i in tqdm(range(0, len(posts), batch_size)):\n",
    "    #    docs = nlp.pipe(df[input_col][i:i+batch_size])\n",
    "    #    df[output_col][i:i+batch_size] = list(docs)\n",
    "    df[output_col] = None\n",
    "    \n",
    "    docs = list()\n",
    "\n",
    "    for i in tqdm(range(0, len(df), batch_size)):\n",
    "        loop_docs = nlp.pipe(df[input_col][i:i+batch_size])\n",
    "        docs += list(loop_docs)\n",
    "\n",
    "    df[output_col] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3490b7a3a2f64bd398c0fff04a8e63c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_spacy_batch(nlp, df, 'doc_spacy_preprocess', 'doc_spacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks a bit faster, ~33% (roughly 20s versus roughly 30s for 1k rows).  Let's functionize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract certain pieces of info from the docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's extract all nouns and do a counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extract nouns\n",
    "def extract_nouns(doc):\n",
    "    patterns = [\"POS:NOUN\"]\n",
    "    spans = extract.matches.token_matches(doc, patterns=patterns)\n",
    "    return [s.lemma_ for s in spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter',\n",
       " 'teaser',\n",
       " 'chapter',\n",
       " 'atleast',\n",
       " 'one',\n",
       " 'other',\n",
       " 'link',\n",
       " 'copyright',\n",
       " 'infringement']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on one row\n",
    "extract_nouns(df.loc[1, 'doc_spacy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37de887fab64ab3a4b388668bd61f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply to all\n",
    "df['nouns'] = df['doc_spacy'].progress_apply(extract_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>in_data</th>\n",
       "      <th>reason_for_exclusion</th>\n",
       "      <th>text</th>\n",
       "      <th>doc_apply</th>\n",
       "      <th>doc_spacy_preprocess</th>\n",
       "      <th>doc_spacy</th>\n",
       "      <th>nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8dasa5</td>\n",
       "      <td>harrypotter</td>\n",
       "      <td>Better idea for Fantastic beast adaptation?</td>\n",
       "      <td>I fell asleep watching the Fantastic Beast mov...</td>\n",
       "      <td>books</td>\n",
       "      <td>harry potter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Better idea for Fantastic beast adaptation? I ...</td>\n",
       "      <td>(Better, idea, for, Fantastic, beast, adaptati...</td>\n",
       "      <td>Better idea for Fantastic beast adaptation? I ...</td>\n",
       "      <td>(Better, idea, for, Fantastic, beast, adaptati...</td>\n",
       "      <td>[idea, beast, adaptation, movie, tv, show, kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6gygi1</td>\n",
       "      <td>dresdenfiles</td>\n",
       "      <td>(No Spoilers) Preview of Peace Talks</td>\n",
       "      <td>Could someone please tell me how many chapters...</td>\n",
       "      <td>books</td>\n",
       "      <td>dresden files</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(No Spoilers) Preview of Peace Talks Could som...</td>\n",
       "      <td>((, No, Spoilers, ), Preview, of, Peace, Talks...</td>\n",
       "      <td>(No Spoilers) Preview of Peace Talks Could som...</td>\n",
       "      <td>((, No, Spoilers, ), Preview, of, Peace, Talks...</td>\n",
       "      <td>[chapter, teaser, chapter, atleast, one, other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5xpvak</td>\n",
       "      <td>Parahumans</td>\n",
       "      <td>Looking for 1-2 players for Worm Campaign</td>\n",
       "      <td>Like the title says im looking for 1-2 more pl...</td>\n",
       "      <td>books</td>\n",
       "      <td>parahumans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Looking for 1-2 players for Worm Campaign Like...</td>\n",
       "      <td>(Looking, for, 1, -, 2, players, for, Worm, Ca...</td>\n",
       "      <td>Looking for 1-2 players for Worm Campaign Like...</td>\n",
       "      <td>(Looking, for, 1, -, 2, players, for, Worm, Ca...</td>\n",
       "      <td>[player, title, player, campaign, system, fill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7p3vuy</td>\n",
       "      <td>Malazan</td>\n",
       "      <td>Just finished TtH</td>\n",
       "      <td>I just finished Toll the Hounds, and I dont kn...</td>\n",
       "      <td>books</td>\n",
       "      <td>malazan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just finished TtH I just finished Toll the Hou...</td>\n",
       "      <td>(Just, finished, TtH, I, just, finished, Toll,...</td>\n",
       "      <td>Just finished TtH I just finished Toll the Hou...</td>\n",
       "      <td>(Just, finished, TtH, I, just, finished, Toll,...</td>\n",
       "      <td>[TtH, book, serie, favorite, writing, thing, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5vkojs</td>\n",
       "      <td>Stormlight_Archive</td>\n",
       "      <td>[TWoK]/[WoR]/[NO SPOILERS] Something that has ...</td>\n",
       "      <td>Here on earth, green/blue eyes are not a domin...</td>\n",
       "      <td>books</td>\n",
       "      <td>cosmere</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[TWoK]/[WoR]/[NO SPOILERS] Something that has ...</td>\n",
       "      <td>([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...</td>\n",
       "      <td>[TWoK]/[WoR]/[NO SPOILERS] Something that has ...</td>\n",
       "      <td>([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...</td>\n",
       "      <td>[TWoK]/[WoR]/[NO, spoiler, spoiler, earth, eye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6vdtnb</td>\n",
       "      <td>Malazan</td>\n",
       "      <td>I'm newish to the series, and this sub</td>\n",
       "      <td>I just wanted to say hi, and that I really lik...</td>\n",
       "      <td>books</td>\n",
       "      <td>malazan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm newish to the series, and this sub I just ...</td>\n",
       "      <td>(I, 'm, newish, to, the, series, ,, and, this,...</td>\n",
       "      <td>I'm newish to the series, and this sub I just ...</td>\n",
       "      <td>(I, 'm, newish, to, the, series, ,, and, this,...</td>\n",
       "      <td>[series, sub, place, community, thing, series,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>67blaq</td>\n",
       "      <td>dresdenfiles</td>\n",
       "      <td>So do you guys think the masquerade is going t...</td>\n",
       "      <td>It seems less and less plausible that humanity...</td>\n",
       "      <td>books</td>\n",
       "      <td>dresden files</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So do you guys think the masquerade is going t...</td>\n",
       "      <td>(So, do, you, guys, think, the, masquerade, is...</td>\n",
       "      <td>So do you guys think the masquerade is going t...</td>\n",
       "      <td>(So, do, you, guys, think, the, masquerade, is...</td>\n",
       "      <td>[guy, masquerade, series, end, humanity, dark,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7uhfal</td>\n",
       "      <td>dresdenfiles</td>\n",
       "      <td>First person narrative...</td>\n",
       "      <td>So I've had a dilemma lately. I love the physi...</td>\n",
       "      <td>books</td>\n",
       "      <td>dresden files</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>First person narrative... So I've had a dilemm...</td>\n",
       "      <td>(First, person, narrative, ..., So, I, 've, ha...</td>\n",
       "      <td>First person narrative... So I've had a dilemm...</td>\n",
       "      <td>(First, person, narrative, ..., So, I, 've, ha...</td>\n",
       "      <td>[person, narrative, dilemma, act, handwriting,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5gpg4i</td>\n",
       "      <td>harrypotter</td>\n",
       "      <td>Were Dementors once human? Theory</td>\n",
       "      <td>To give a bit of backstory, Dementors were fir...</td>\n",
       "      <td>books</td>\n",
       "      <td>harry potter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Were Dementors once human? Theory To give a bi...</td>\n",
       "      <td>(Were, Dementors, once, human, ?, Theory, To, ...</td>\n",
       "      <td>Were Dementors once human? Theory To give a bi...</td>\n",
       "      <td>(Were, Dementors, once, human, ?, Theory, To, ...</td>\n",
       "      <td>[theory, bit, backstory, dementor, unveiling, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7ca4d7</td>\n",
       "      <td>eroticauthors</td>\n",
       "      <td>Does anybody have any experience writing espio...</td>\n",
       "      <td>My latest short was a hardcore, mind control l...</td>\n",
       "      <td>books</td>\n",
       "      <td>erotic fiction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Does anybody have any experience writing espio...</td>\n",
       "      <td>(Does, anybody, have, any, experience, writing...</td>\n",
       "      <td>Does anybody have any experience writing espio...</td>\n",
       "      <td>(Does, anybody, have, any, experience, writing...</td>\n",
       "      <td>[experience, espionage, spy, erotica, mind, st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id           subreddit   \n",
       "0  8dasa5         harrypotter  \\\n",
       "1  6gygi1        dresdenfiles   \n",
       "2  5xpvak          Parahumans   \n",
       "3  7p3vuy             Malazan   \n",
       "4  5vkojs  Stormlight_Archive   \n",
       "5  6vdtnb             Malazan   \n",
       "6  67blaq        dresdenfiles   \n",
       "7  7uhfal        dresdenfiles   \n",
       "8  5gpg4i         harrypotter   \n",
       "9  7ca4d7       eroticauthors   \n",
       "\n",
       "                                               title   \n",
       "0        Better idea for Fantastic beast adaptation?  \\\n",
       "1               (No Spoilers) Preview of Peace Talks   \n",
       "2          Looking for 1-2 players for Worm Campaign   \n",
       "3                                  Just finished TtH   \n",
       "4  [TWoK]/[WoR]/[NO SPOILERS] Something that has ...   \n",
       "5             I'm newish to the series, and this sub   \n",
       "6  So do you guys think the masquerade is going t...   \n",
       "7                          First person narrative...   \n",
       "8                  Were Dementors once human? Theory   \n",
       "9  Does anybody have any experience writing espio...   \n",
       "\n",
       "                                            selftext category_1   \n",
       "0  I fell asleep watching the Fantastic Beast mov...      books  \\\n",
       "1  Could someone please tell me how many chapters...      books   \n",
       "2  Like the title says im looking for 1-2 more pl...      books   \n",
       "3  I just finished Toll the Hounds, and I dont kn...      books   \n",
       "4  Here on earth, green/blue eyes are not a domin...      books   \n",
       "5  I just wanted to say hi, and that I really lik...      books   \n",
       "6  It seems less and less plausible that humanity...      books   \n",
       "7  So I've had a dilemma lately. I love the physi...      books   \n",
       "8  To give a bit of backstory, Dementors were fir...      books   \n",
       "9  My latest short was a hardcore, mind control l...      books   \n",
       "\n",
       "       category_2 category_3  in_data reason_for_exclusion   \n",
       "0    harry potter        NaN     True                  NaN  \\\n",
       "1   dresden files        NaN     True                  NaN   \n",
       "2      parahumans        NaN     True                  NaN   \n",
       "3         malazan        NaN     True                  NaN   \n",
       "4         cosmere        NaN     True                  NaN   \n",
       "5         malazan        NaN     True                  NaN   \n",
       "6   dresden files        NaN     True                  NaN   \n",
       "7   dresden files        NaN     True                  NaN   \n",
       "8    harry potter        NaN     True                  NaN   \n",
       "9  erotic fiction        NaN     True                  NaN   \n",
       "\n",
       "                                                text   \n",
       "0  Better idea for Fantastic beast adaptation? I ...  \\\n",
       "1  (No Spoilers) Preview of Peace Talks Could som...   \n",
       "2  Looking for 1-2 players for Worm Campaign Like...   \n",
       "3  Just finished TtH I just finished Toll the Hou...   \n",
       "4  [TWoK]/[WoR]/[NO SPOILERS] Something that has ...   \n",
       "5  I'm newish to the series, and this sub I just ...   \n",
       "6  So do you guys think the masquerade is going t...   \n",
       "7  First person narrative... So I've had a dilemm...   \n",
       "8  Were Dementors once human? Theory To give a bi...   \n",
       "9  Does anybody have any experience writing espio...   \n",
       "\n",
       "                                           doc_apply   \n",
       "0  (Better, idea, for, Fantastic, beast, adaptati...  \\\n",
       "1  ((, No, Spoilers, ), Preview, of, Peace, Talks...   \n",
       "2  (Looking, for, 1, -, 2, players, for, Worm, Ca...   \n",
       "3  (Just, finished, TtH, I, just, finished, Toll,...   \n",
       "4  ([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...   \n",
       "5  (I, 'm, newish, to, the, series, ,, and, this,...   \n",
       "6  (So, do, you, guys, think, the, masquerade, is...   \n",
       "7  (First, person, narrative, ..., So, I, 've, ha...   \n",
       "8  (Were, Dementors, once, human, ?, Theory, To, ...   \n",
       "9  (Does, anybody, have, any, experience, writing...   \n",
       "\n",
       "                                doc_spacy_preprocess   \n",
       "0  Better idea for Fantastic beast adaptation? I ...  \\\n",
       "1  (No Spoilers) Preview of Peace Talks Could som...   \n",
       "2  Looking for 1-2 players for Worm Campaign Like...   \n",
       "3  Just finished TtH I just finished Toll the Hou...   \n",
       "4  [TWoK]/[WoR]/[NO SPOILERS] Something that has ...   \n",
       "5  I'm newish to the series, and this sub I just ...   \n",
       "6  So do you guys think the masquerade is going t...   \n",
       "7  First person narrative... So I've had a dilemm...   \n",
       "8  Were Dementors once human? Theory To give a bi...   \n",
       "9  Does anybody have any experience writing espio...   \n",
       "\n",
       "                                           doc_spacy   \n",
       "0  (Better, idea, for, Fantastic, beast, adaptati...  \\\n",
       "1  ((, No, Spoilers, ), Preview, of, Peace, Talks...   \n",
       "2  (Looking, for, 1, -, 2, players, for, Worm, Ca...   \n",
       "3  (Just, finished, TtH, I, just, finished, Toll,...   \n",
       "4  ([, TWoK]/[WoR]/[NO, SPOILERS, ], Something, t...   \n",
       "5  (I, 'm, newish, to, the, series, ,, and, this,...   \n",
       "6  (So, do, you, guys, think, the, masquerade, is...   \n",
       "7  (First, person, narrative, ..., So, I, 've, ha...   \n",
       "8  (Were, Dementors, once, human, ?, Theory, To, ...   \n",
       "9  (Does, anybody, have, any, experience, writing...   \n",
       "\n",
       "                                               nouns  \n",
       "0  [idea, beast, adaptation, movie, tv, show, kno...  \n",
       "1  [chapter, teaser, chapter, atleast, one, other...  \n",
       "2  [player, title, player, campaign, system, fill...  \n",
       "3  [TtH, book, serie, favorite, writing, thing, b...  \n",
       "4  [TWoK]/[WoR]/[NO, spoiler, spoiler, earth, eye...  \n",
       "5  [series, sub, place, community, thing, series,...  \n",
       "6  [guy, masquerade, series, end, humanity, dark,...  \n",
       "7  [person, narrative, dilemma, act, handwriting,...  \n",
       "8  [theory, bit, backstory, dementor, unveiling, ...  \n",
       "9  [experience, espionage, spy, erotica, mind, st...  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12fe63d2e6534ad584981732a7dd3252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0      None\n",
       "1      None\n",
       "2      None\n",
       "3      None\n",
       "4      None\n",
       "       ... \n",
       "995    None\n",
       "996    None\n",
       "997    None\n",
       "998    None\n",
       "999    None\n",
       "Name: nouns, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do a noun counter\n",
    "noun_counter = Counter()\n",
    "\n",
    "df['nouns'].progress_map(noun_counter.update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('book', 1085),\n",
       " ('time', 445),\n",
       " ('thing', 355),\n",
       " ('story', 355),\n",
       " ('series', 340),\n",
       " ('character', 247),\n",
       " ('world', 233),\n",
       " ('people', 231),\n",
       " ('power', 222),\n",
       " ('way', 219),\n",
       " ('spoiler', 212),\n",
       " ('question', 198),\n",
       " ('movie', 196),\n",
       " ('name', 163),\n",
       " ('year', 158),\n",
       " ('day', 148),\n",
       " ('lot', 144),\n",
       " ('point', 138),\n",
       " ('chapter', 137),\n",
       " ('idea', 136)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 20 nouns\n",
    "noun_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, let's extract all noun-verb sequences and do a counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extract nouns\n",
    "def extract_noun_verbs(doc):\n",
    "    patterns = [\"POS:NOUN:+ POS:VERB:+\", \"POS:VERB:+ POS:NOUN:+\"]\n",
    "    spans = extract.matches.token_matches(doc, patterns=patterns)\n",
    "    return ['_'.join([w.lemma_ for w in s]) for s in spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moment_involve']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on one row\n",
    "extract_noun_verbs(df.loc[0, 'doc_spacy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae92e1e5660a4f158214b1c434e79b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply to all\n",
    "df['noun_verbs'] = df['doc_spacy'].progress_apply(extract_noun_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b6eee27613405d82c518806ba1cb60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0      None\n",
       "1      None\n",
       "2      None\n",
       "3      None\n",
       "4      None\n",
       "       ... \n",
       "995    None\n",
       "996    None\n",
       "997    None\n",
       "998    None\n",
       "999    None\n",
       "Name: noun_verbs, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do a noun counter\n",
    "noun_verbs_counter = Counter()\n",
    "\n",
    "df['noun_verbs'].progress_map(noun_verbs_counter.update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('make_sense', 23),\n",
       " ('guy_think', 15),\n",
       " ('take_place', 14),\n",
       " ('have_trouble', 11),\n",
       " ('read_book', 10),\n",
       " ('page_read', 8),\n",
       " ('thing_happen', 7),\n",
       " ('lock_chest', 7),\n",
       " ('book_have', 7),\n",
       " ('have_access', 7),\n",
       " ('people_say', 7),\n",
       " ('make_thing', 6),\n",
       " ('thrice_lock', 6),\n",
       " ('book_set', 6),\n",
       " ('thing_go', 6),\n",
       " ('see_people', 5),\n",
       " ('write_erotica', 5),\n",
       " ('make_money', 5),\n",
       " ('character_seem', 5),\n",
       " ('question_regard', 5)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 20 nouns\n",
    "noun_verbs_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result BEFORE training:\n",
      "     Festy GPE\n",
      "   Training ...\n",
      "Result AFTER training:\n",
      "     Festy DISTRICT\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/69181078/spacy-how-do-you-add-custom-ner-labels-to-a-pre-trained-model\n",
    "\n",
    "import spacy\n",
    "import random\n",
    "from spacy import util\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example\n",
    "from spacy.language import Language\n",
    "\n",
    "def print_doc_entities(_doc: Doc):\n",
    "    if _doc.ents:\n",
    "        for _ent in _doc.ents:\n",
    "            print(f\"     {_ent.text} {_ent.label_}\")\n",
    "    else:\n",
    "        print(\"     NONE\")\n",
    "\n",
    "def customizing_pipeline_component(nlp: Language):\n",
    "    # NOTE: Starting from Spacy 3.0, training via Python API was changed. For information see - https://spacy.io/usage/v3#migrating-training-python\n",
    "    train_data = [\n",
    "        ('We need to deliver it to Festy.', [(25, 30, 'DISTRICT')]),\n",
    "        ('I like red oranges', [])\n",
    "    ]\n",
    "\n",
    "    # Result before training\n",
    "    print(f\"\\nResult BEFORE training:\")\n",
    "    doc = nlp(u'I need a taxi to Festy.')\n",
    "    print_doc_entities(doc)\n",
    "\n",
    "    # Disable all pipe components except 'ner'\n",
    "    disabled_pipes = []\n",
    "    for pipe_name in nlp.pipe_names:\n",
    "        if pipe_name != 'ner':\n",
    "            nlp.disable_pipes(pipe_name)\n",
    "            disabled_pipes.append(pipe_name)\n",
    "\n",
    "    print(\"   Training ...\")\n",
    "    optimizer = nlp.create_optimizer()\n",
    "    for _ in range(25):\n",
    "        random.shuffle(train_data)\n",
    "        for raw_text, entity_offsets in train_data:\n",
    "            doc = nlp.make_doc(raw_text)\n",
    "            example = Example.from_dict(doc, {\"entities\": entity_offsets})\n",
    "            nlp.update([example], sgd=optimizer)\n",
    "\n",
    "    # Enable all previously disabled pipe components\n",
    "    for pipe_name in disabled_pipes:\n",
    "        nlp.enable_pipe(pipe_name)\n",
    "\n",
    "    # Result after training\n",
    "    print(f\"Result AFTER training:\")\n",
    "    doc = nlp(u'I need a taxi to Festy.')\n",
    "    print_doc_entities(doc)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "customizing_pipeline_component(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "for i in range(0, len(posts), batch_size):\n",
    "    docs = nlp.pipe(posts['selftext'][i:i+batch_size])\n",
    "    \n",
    "    for j, doc in enumerate(docs):\n",
    "        for col, values in extract_nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, test out examples from a sentiment repo\n",
    "\n",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) negative 0.7236\n",
      "2) neutral 0.2287\n",
      "3) positive 0.0477\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "#model.save_pretrained(MODEL)\n",
    "text = \"Covid cases are increasing fast!\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "# # TF\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "# text = \"Covid cases are increasing fast!\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)\n",
    "# scores = output[0][0].numpy()\n",
    "# scores = softmax(scores)\n",
    "# Print labels and scores\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = config.id2label[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying ammr/RobertaLarge-Gambling6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "ammr/RobertaLarge-Gambling6.2 does not appear to have a file named config.json. Checkout 'https://huggingface.co/ammr/RobertaLarge-Gambling6.2/main' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/c/Users/Kenneth/Documents/GitHub/python_playground/modelling/huggingface/huggingface/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py:259\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/mnt/c/Users/Kenneth/Documents/GitHub/python_playground/modelling/huggingface/huggingface/lib/python3.9/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/ammr/RobertaLarge-Gambling6.2/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/c/Users/Kenneth/Documents/GitHub/python_playground/modelling/huggingface/huggingface/lib/python3.9/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m/mnt/c/Users/Kenneth/Documents/GitHub/python_playground/modelling/huggingface/huggingface/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/Kenneth/Documents/GitHub/python_playground/modelling/huggingface/huggingface/lib/python3.9/site-packages/huggingface_hub/file_download.py:1134\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/Kenneth/Documents/GitHub/python_playground/modelling/huggingface/huggingface/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/Kenneth/Documents/GitHub/python_playground/modelling/huggingface/huggingface/lib/python3.9/site-packages/huggingface_hub/file_download.py:1475\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1466\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1467\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1468\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1473\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1474\u001b[0m )\n\u001b[0;32m-> 1475\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/Kenneth/Documents/GitHub/python_playground/modelling/huggingface/huggingface/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py:269\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    268\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EntryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGatedRepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-64177381-0c1ce3ed74278f52368b9d40)\n\nEntry Not Found for url: https://huggingface.co/ammr/RobertaLarge-Gambling6.2/resolve/main/config.json.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mammr/RobertaLarge-Gambling6.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL)\n",
      "File \u001b[0;32m/mnt/c/Users/Kenneth/Documents/GitHub/python_playground/modelling/huggingface/huggingface/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:634\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m--> 634\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m     config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[0;32m/mnt/c/Users/Kenneth/Documents/GitHub/python_playground/modelling/huggingface/huggingface/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:896\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    894\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m    895\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 896\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m/mnt/c/Users/Kenneth/Documents/GitHub/python_playground/modelling/huggingface/huggingface/lib/python3.9/site-packages/transformers/configuration_utils.py:573\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 573\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    575\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/mnt/c/Users/Kenneth/Documents/GitHub/python_playground/modelling/huggingface/huggingface/lib/python3.9/site-packages/transformers/configuration_utils.py:628\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/Kenneth/Documents/GitHub/python_playground/modelling/huggingface/huggingface/lib/python3.9/site-packages/transformers/utils/hub.py:454\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m         revision \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    456\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    457\u001b[0m     )\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# First we try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m try_to_load_from_cache(path_or_repo_id, full_filename, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir, revision\u001b[38;5;241m=\u001b[39mrevision)\n",
      "\u001b[0;31mOSError\u001b[0m: ammr/RobertaLarge-Gambling6.2 does not appear to have a file named config.json. Checkout 'https://huggingface.co/ammr/RobertaLarge-Gambling6.2/main' for available files."
     ]
    }
   ],
   "source": [
    "MODEL = f\"ammr/RobertaLarge-Gambling6.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.auto.tokenization_auto because of the following error (look up to see its traceback):\nlibssl.so.10: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.7/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1125\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mdynamic_module_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_class_from_dynamic_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mtokenization_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m from .tokenization_utils_base import (\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mENCODE_KWARGS_DOCSTRING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tokenizers_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtokenizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAddedToken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtokenizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoding\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mEncodingFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.7/site-packages/tokenizers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m from .tokenizers import (\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libssl.so.10: cannot open shared object file: No such file or directory",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2671/2739314703.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFAutoModelForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.7/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.7/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.7/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             ) from e\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.auto.tokenization_auto because of the following error (look up to see its traceback):\nlibssl.so.10: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "#model.save_pretrained(MODEL)\n",
    "text = \"Covid cases are increasing fast!\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "# # TF\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "# text = \"Covid cases are increasing fast!\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)\n",
    "# scores = output[0][0].numpy()\n",
    "# scores = softmax(scores)\n",
    "# Print labels and scores\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = config.id2label[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm-up: numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 415.03803869054195\n",
      "199 294.89318640576624\n",
      "299 210.34355120956133\n",
      "399 150.82212187163634\n",
      "499 108.90683215126217\n",
      "599 79.38105441571744\n",
      "699 58.5767795388155\n",
      "799 43.91390394828215\n",
      "899 33.57690771092599\n",
      "999 26.287842128834114\n",
      "1099 21.14686557304085\n",
      "1199 17.52017966501547\n",
      "1299 14.961243719728937\n",
      "1399 13.15536359648386\n",
      "1499 11.880705767485411\n",
      "1599 10.980858201652115\n",
      "1699 10.34551177639144\n",
      "1799 9.896854740008397\n",
      "1899 9.579987903168718\n",
      "1999 9.356170479379067\n",
      "Result: y = -0.024309887664472608 + 0.8534384488443799 x + 0.004193860140398336 x^2 + -0.09286065221896801 x^3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 3893.76416015625\n",
      "199 2734.6689453125\n",
      "299 1922.3388671875\n",
      "399 1352.7203369140625\n",
      "499 953.0848388671875\n",
      "599 672.5684814453125\n",
      "699 475.5719909667969\n",
      "799 337.1661682128906\n",
      "899 239.88427734375\n",
      "999 171.4793701171875\n",
      "1099 123.36146545410156\n",
      "1199 89.50177764892578\n",
      "1299 65.66727447509766\n",
      "1399 48.88423538208008\n",
      "1499 37.06294631958008\n",
      "1599 28.734058380126953\n",
      "1699 22.86427116394043\n",
      "1799 18.726476669311523\n",
      "1899 15.808906555175781\n",
      "1999 13.751249313354492\n",
      "Result: y = 0.07287223637104034 + 0.8431879878044128 x + -0.012571671977639198 x^2 + -0.09140260517597198 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Tensors and autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2132.51904296875\n",
      "199 1510.9560546875\n",
      "299 1071.3453369140625\n",
      "399 760.41064453125\n",
      "499 540.4814453125\n",
      "599 384.9165954589844\n",
      "699 274.87603759765625\n",
      "799 197.03587341308594\n",
      "899 141.97201538085938\n",
      "999 103.01905059814453\n",
      "1099 75.46246337890625\n",
      "1199 55.96778106689453\n",
      "1299 42.17603302001953\n",
      "1399 32.41878890991211\n",
      "1499 25.515701293945312\n",
      "1599 20.631790161132812\n",
      "1699 17.176414489746094\n",
      "1799 14.731683731079102\n",
      "1899 13.00197982788086\n",
      "1999 11.778152465820312\n",
      "Result: y = 0.05754031613469124 + 0.8591004014015198 x + -0.009926660917699337 x^2 + -0.09366601705551147 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Defining new autograd functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 209.95834350585938\n",
      "199 144.66018676757812\n",
      "299 100.70249938964844\n",
      "399 71.03519439697266\n",
      "499 50.97850799560547\n",
      "599 37.403133392333984\n",
      "699 28.206867218017578\n",
      "799 21.973188400268555\n",
      "899 17.7457275390625\n",
      "999 14.877889633178711\n",
      "1099 12.931766510009766\n",
      "1199 11.610918045043945\n",
      "1299 10.714258193969727\n",
      "1399 10.10548210144043\n",
      "1499 9.692106246948242\n",
      "1599 9.411375045776367\n",
      "1699 9.220745086669922\n",
      "1799 9.091285705566406\n",
      "1899 9.003361701965332\n",
      "1999 8.943639755249023\n",
      "Result: y = -5.423830273798558e-09 + -2.208526849746704 * P3(1.3320399228078372e-09 + 0.2554861009120941 x)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For this example, we need\n",
    "# 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized\n",
    "# not too far from the correct result to ensure convergence.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'P3'.\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # P3 using our custom autograd operation.\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 489.1260986328125\n",
      "199 326.472412109375\n",
      "299 218.90310668945312\n",
      "399 147.7622833251953\n",
      "499 100.712890625\n",
      "599 69.59596252441406\n",
      "699 49.01630401611328\n",
      "799 35.405311584472656\n",
      "899 26.40318489074707\n",
      "999 20.44918441772461\n",
      "1099 16.51112937927246\n",
      "1199 13.906396865844727\n",
      "1299 12.18358039855957\n",
      "1399 11.044008255004883\n",
      "1499 10.290224075317383\n",
      "1599 9.791631698608398\n",
      "1699 9.461831092834473\n",
      "1799 9.243647575378418\n",
      "1899 9.09931755065918\n",
      "1999 9.003837585449219\n",
      "Result: y = 0.0013163856929168105 + 0.8435145616531372 x + -0.00022709897893946618 x^2 + -0.0914490669965744 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 25590.3203125\n",
      "199 10892.1796875\n",
      "299 3849.25\n",
      "399 978.232177734375\n",
      "499 202.438232421875\n",
      "599 82.48208618164062\n",
      "699 50.78310775756836\n",
      "799 30.701852798461914\n",
      "899 19.048215866088867\n",
      "999 13.197745323181152\n",
      "1099 10.427064895629883\n",
      "1199 9.196508407592773\n",
      "1299 8.853813171386719\n",
      "1399 8.817937850952148\n",
      "1499 8.817169189453125\n",
      "1599 8.817171096801758\n",
      "1699 8.927461624145508\n",
      "1799 9.17581558227539\n",
      "1899 9.063740730285645\n",
      "1999 8.94500732421875\n",
      "Result: y = 0.0007251655333675444 + 0.8562337160110474 x + 0.0007269359775818884 x^2 + -0.09383764863014221 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Prepare the input tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use RMSprop; the optim package contains many other\n",
    "# optimization algorithms. The first argument to the RMSprop constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "linear_layer = model[0]\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Custom nn Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 3708.881103515625\n",
      "199 2468.661376953125\n",
      "299 1644.7138671875\n",
      "399 1097.1524658203125\n",
      "499 733.1485595703125\n",
      "599 491.08563232421875\n",
      "699 330.0555114746094\n",
      "799 222.89071655273438\n",
      "899 151.54428100585938\n",
      "999 104.0241470336914\n",
      "1099 72.35916900634766\n",
      "1199 51.24930953979492\n",
      "1299 37.16911697387695\n",
      "1399 27.77261734008789\n",
      "1499 21.49852752685547\n",
      "1599 17.30673599243164\n",
      "1699 14.504561424255371\n",
      "1799 12.630081176757812\n",
      "1899 11.375333786010742\n",
      "1999 10.534866333007812\n",
      "Result: y = 0.021114975214004517 + 0.8214263319969177 x + -0.0036426838487386703 x^2 + -0.0883072018623352 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate four parameters and assign them as\n",
    "        member parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = Polynomial3()\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the nn.Linear\n",
    "# module which is members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "for t in range(2000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Control Flow + Weight Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 2329.285888671875\n",
      "3999 1092.639892578125\n",
      "5999 572.0111694335938\n",
      "7999 257.60223388671875\n",
      "9999 131.4246368408203\n",
      "11999 65.56714630126953\n",
      "13999 34.894676208496094\n",
      "15999 21.250364303588867\n",
      "17999 14.811595916748047\n",
      "19999 11.761395454406738\n",
      "21999 10.094457626342773\n",
      "23999 9.536259651184082\n",
      "25999 9.160627365112305\n",
      "27999 8.763763427734375\n",
      "29999 8.690313339233398\n",
      "Result: y = 0.009464428760111332 + 0.8544765710830688 x + -0.0022061513736844063 x^2 + -0.09323178976774216 x^3 + 9.393560321768746e-05 x^4 ? + 9.393560321768746e-05 x^5 ?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate five parameters and assign them as members.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 4, 5\n",
    "        and reuse the e parameter to compute the contribution of these orders.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same parameter many\n",
    "        times when defining a computational graph.\n",
    "        \"\"\"\n",
    "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y = y + self.e * x ** exp\n",
    "        return y\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet()\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
    "for t in range(30000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 2000 == 1999:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
